kubernets deployments examples - https://github.com/kubernetes/examples
kubectl exec -it <podname> -- /bin/bash - to go inside the pod
MASTER NODE COMPONENTS
API server
kube controller
sheduler 
etcd database
cloud controller manager

WORKER NODE 
kubelet 
kube proxy
container runtime

https://kubernetes.io/ - kubernetes documentation

EFK stands for Elasticsearch, Fluent bit, and Kibana. Elasticsearch is a scalable and distributed search engine that is commonly used to store large amounts of log data. It is a NoSQL database. Its primary function is to store and retrieve logs from fluent bit.2

To create any objects(pod/deployment/service and etc) in kubernetes use manifest files
apiVersion
kind
metadata
spec

if there are two containers in pod, they can communicate with network namespace
when we create pod using manifest file, pod only gets the IP, but not the conatiner



kubectl get all
kubectl get all -o wide
kubectl get service
kubectl get namespaces
kubectl get nodes
kubectl get pods -n <namespace-name> if we created in the namespace
kubectl get pods -o wide (displays on which node it got created)
kubectl get pods --watch
kubectl create -f pod.yml
kubectl describe pod <nameofthepod>
kubectl exec -it <pod-name> -- /bin/bash - go inside the pod, like goin' inside docker conatiner
kubectl exec -it -c <pod-name> -- /bin/bash - to login to the specific container in a pod
kubectl delete pod <podname>
kubectl delete -f file.yml

kubectl scale rc <rc-name> --replicas=5 (we can scale the cluster with replication controller, on the cli) or kubectl edit rc <replication-controller name>
if we won't mention selector then replication controller won't do scale out/in
But we won't use replication controller or replicaset to deploy our apps in realtime

stateless means the application won't gonna save the data, so if container or pod dies, we won't bother about data, if we expect the data that is stateful application
Majorly we use deploymnets in realtime(Recreate, rolling-update, Blue/green, canary) - we use this when we upgrade the application without downtme
in deploymnets the replicaset is there by default
kubectl describe deployment nginx-deployment
kubectl get deployment
kubectl get replicaset (it's automatically created when deployment is created)
 Types of deploying applications replicaset, replication controller, deployments, daemonsets, statefulsets


SERVICES
services work with the concept called selector
kubectl get service (service selector name and pod label name should be matched for the perfect connection)
ClusterIP - we use internally, can't be exposed to the external world, works within the cluster(curl http://<ipofpod>(kubectl get po -o wide/kubectl describe po <podname>))
NodePort - if we wanna access our application from external world, we need to configure service type called NodePort(30000-32767)(it's not that better for HA) - this is not recommended for production traffic, we should use the load balancer
LoadBalancer

NAMESPACES
kubectl get namespace
kuebectl get pods -n <name-of-the namespace>
kuebectl get all -n <name-of-the namespace>
kubectl create namespace <name-of-the-namespace>



configmaps is in plain text format
secrets is in encoded format

CONFIGMAPS
can be created using directories, files and literals
kubectl get cm 
kubectl get cm <configmap-file-name> -o yaml (gives the file output to yaml)


HELM
Helm is the pachage manager for kubernetes
https://helm.sh/ - helm official website



PersistentVolume PersistentVolumeClaims
kubectl get pv
kubectl get pvc
kubectl get pv, pvc (we check this together as well)



HORIZONTAL POD AUTOSCALER(it also the also has the cooling period, suppose if there is a decrease of load, there is a coolng period time, to scale in the pod, it should elapse the cooldown period)
kubectl get hpa
kubectl describe hpa <name-of-the-hpa>
we can configure for cpu, network utilizaton and ram(based on the metrics)
kubectl top pods -n <namespace-name>(shows the uasge of metrics)


CENTRALIZED LOCATION FOR LOGS(EFK ELASTICSEARCH, FLUENTD AND KIBANA)
kubectl get pods
kubectl logs <pod-name> (standard output, which prints onto the console), but logs need to be redirected to a logfile

elasticsearch is a database
fluentd resides in the worker nodes and send the application log files info to elasticsearch
kibana visualization tool fetches the log data from elasticsearch and displays the on the dashboard
can be deployed using the helm charts
These three get deployed as pods
fluentd is available in all worker nodes



For Daemonsets, replicas won't work.
sidecar container




==========================================================================================================

kubectl - cli tool to interact with eks on aws form our mac
kubectl version --short --client - helps know the client
eksctl - helps creating eks cluster on aws
eksctl version - helps know the client

========================================
Create your cluster and nodes

eksctl create cluster --name cluster-name  \
--region region-name \
--node-type instance-type \
--nodes-min 2 \
--nodes-max 2 \ 
--zones <AZ-1>,<AZ-2>
example:
eksctl create cluster --name valaxy-cluster \
--region ap-south-1 \
--node-type t2.small  - no need of back slash at the end of command if it's a last line

eksctl create cluster --name sudhamsh-eks --region us-west-2 --node-type t2.small

connect to the cluster
aws eks --region <your-cluster-region> update-kubeconfig --name <your-cluster-name>

deleting the cluster
eksctl delete cluster <cluster-name> --region<region-name>

==========================================
To delete the EKS clsuter
eksctl delete cluster valaxy --region ap-south-1


Validate your cluster using by creating by checking nodes and by creating a pod
kubectl get nodes
kubectl run pod tomcat --image=tomcat 


=================
eksctl get cluster --region us-west-2
kubectl apply -f file.yaml - manifest/defintion files 
kubectl get nodes -here nodes means worker nodes (worker nodes maeans ec2 instances running under the eks cluster)
kubectl get pods -A (helps displaying all pods in all namespaces)
kubectl get pods (helps shows the pods in the current namespace)
kubectl get pods -o wide
kubectl get nodes -o wide(displays the workernodes(ec2-instance's public and private ip))
kubectl get deployments
kubectl get replicasets
kubectl get service
kubectl run pod <arbitary> --image=tomcat --port=80
kubectl describe pod <name-of-the-pod>
kubectl port-forward <pod_name> <local_port>:<pod_port>
kubectl get all -A (helps displaying all the workloads)
kubectl describe pod <podn-ame> (here describe is like inspect in docker)
kubectl describe deploy <deployments-name> 
kubectl describe service <service-name>
kubectl describe replicaset <replicaset-name>
kubectl edit deployment <deployment-name>  - it opens the vim editor, we can edit the copies of pods we desired(we can edit the stuff on the fly)

kubectl delete pod<pod_name>
kubectl delete -f . deletes all the stuff in a go

kubectl get pods -w - helps letting know the status of pod i.e., terminating/running/pending(here w indicates wait, reflects the activity status of pod)
kubectl get svc -w - helps reflecting in the stuff on the go

ingress controller(fantastic-stuff) installation hierarchy
kubectl create namespace ingress-nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml
kubectl get all -n ingress-nginx (displays pods/deployments/Service in the namespace of ingress-nginx)
kubectl get ingress
kubectl get service -n ingress-nginx

configmap
kubectl get configmap
kubectl describe cm <configmap-name>
kubectl get secret
kubectl describe secret

Namespace
kubectl get namespaces
kubectl get all -n kube-system (-n refers the namespace)(here all includes pod/deployment/service/configmaps/service)
kubectl get all -n default/ kubectl get all(by default the commands executed from default namespace)
kubectl create namespace <arbitary-name>
==============================






kubectl get all -A
kubectl edit deploy my-deployment - opens the vi editor, this command is used edit the deployment on the fly
kubectl get pods -w (w stands for wait/watch, prints background activity of pod)
kubectl get nodes -o wide displays the worker-node/ec2 private and external ip(the external/public-IP of worker node and and the nodeport portnumber number i.e., 30000-32767, for example public-ip of ec2 instance and port number of nodeport)
kubectl get po -o wide - displays the pod Ips
kubectl delete po <pod-name>


pv, pvc and storage

pod>pvc>pv

consider a case where we mysql db pod which our application pod uses, where our data can be added, modified in the db(refer the bottom line)
when you restart the db pod all the changes will be gone, there is no data persistence outta the box.

storage doesn't depend on the pod lifecycle
However, we donno, on which node the pod gets restarted storage must be available on all nodes
storage needs to survive if the cluster crashes

another use case for persistent storage isn't for db but for a directory
Maybe you've  a application that writes/reads the files from pre-configured directory(this could be session files or configuration files), you can configure any of these type of storage using kubernetes component called pv
Think of a persistent volume as a cluster resource just like ram/cpu that used to store data
PV gets created just like any other component using yaml file(kind:PersistentVolume), PV is just a abstraction component it needs actual physical storage like local disk, nfs, or cloud storage(block storage)
where does this storage come from and who makes it available to the cluster

pvc must be in the same namespace, but while coming to the pv it's not supposed to be in the specific namespace

Auto healing - without the manual intervention of a engineer the container should come up (replicaset helps automates this for us)




Abhishek - k8s

WORKER NODE COMPONENTS - executes the actions
container runtime - which is responsible for running your container, execution enevironment for a container
kubelet is responsible for creation of pods and ensures the pod is always in running state, if a pod isn't running kubelet informs the API-gateway
kube-proxy basically provides networking, every pod that we're creating it should be allocated with IP address(we can think of ClusterIP), (provides networking, ip addresses and default load balancing capability)

MASTER NODE COMPONENTS - controls the actions
API-server - exposes the kubernetes, basically takes all the requests from external world
sheduler - helps scheduling the pod in a available worker nodes(scheduler receives the info from API-server)
etcd  - basically a key-value store db (stores the info of the cluster)
controller manager - maintains the state using replicasets
ccm(cloud-controller-manager-eks)


pod can have one or more than one container(sidecar, init containers - these basically supports the actual container, suppose you've deployed application in a container, and the container wanna read some config files/user related files from another conatiner, or for logging)

No need of byhearting the kubernetes yaml stuff, just aware of where you wanna modify your keys


we can ssh into the pod using 
kubectl exec -it <podname> -- /bin/bash 

we can also check the status of a po using
kubectl describe pod <podname> it gives the events, on what image pod is running, on which node it's running, what volumes was attached to it

we can also check the logs using 
kubectl logs <podname>

if we wanna curl the ip of a pod ssh into the worker node > curl <pod-ip>

Deployment - it has the capabilities of auto-healing, auto-scaling and ensures zero downtime


Replicaset is a kubernetes controller(which maintains the state, this ensures the auto-healing capability)

kubectl get all

kubectl get all -A - list the resources from all the namespaces

kubectl get rs - for replicaset

deployment creates a replicaset and replicaset create a pod for us

sudhamshapp/reagapp:latest



pod ideal count depends on the number of concurrent users, for example, if a pod handle the ten requests, for 100 users we need atleast the 10 pods

services won't rely on pod IPs as they're dynamic because of pod lifecycle, it relies on pod's labels
ClusterIP(inside kubernetes cluster) - inside the kubernetes cluster
NodePort(accessed via workernodes:(nodeport-Ips)) - to be accessed by your organization or within our network
LoadBalancer(external world)

kubeshark - practical traffic viewing, helps how traffic is flowing within kubernetes and how one component is talking to the other component

ingress(ingress controller and ingress resource)(acts as a layer7 load balancer)
we're exposing multiple services(NodePort, ClusterIP, LoadBalancer)(behind the services ofcourse we have the deployments>replicasets>pods) with one ingress on the different paths
when we opt the service type as LoadBalancer in kubernetes, it provisions the classic load balancer which is a legacy one

For example if we do a deployment using microsoft.com to access it we should create a service 
For example if we do a deployment using google.com to access it we should create a service 
For example if we do a deployment using amazon.com to access it we should create a service 
if above are running as microservices, the requests coming from external world based on the url and path, the ingress picks up the url and forwards to the correct service deployment

ingress resource bridges the gap b/w ingress controller and actual deployments

The highlight stuff is  both ingress controller and ingress resource has the same load balancer i.e., classic load balncer
# kubectl get svc -n ingress-nginx - to check the ingress controller
# kubectl get ingress - to check the ingress service

Although we've two applications i.e., nginx and httpd, it created one load balancer instead of two load balancers that is the beauty of ingress controller, we can access the different deployments using a single loadbalancerIP/nginx, loadbalancerIP/httpd 


Request comes from external-world>ingress-controller>reads the stuff of ingress-resource> then forwards the traffic to specific deployment



CONFIGMAPS(non-sensitive-data) & SECRETS(sensitive-data)

How can we gonna refer the configmap/secrets in a pod

for the secrets data is encrypted at rest and for the secrets we can enforce a strong rbac(for the entire resource in kubernetes only a devops engineer should have access to it)

kubectl get cm
kubectl describe cm <cm-name>

kubectl exec -it <podname> -- /bin/bash - to go inside the pod - takes inside the pod
env | grep DB - to retrieve specific value that we used in the configmap

changing the environment variables inside the containers isn't possible without a restart, but in the production we shouldn't restart the container, that's why use volume mounts as "files" because we're mounting right, where configmap info saved in a file and developers can read the information from file instead of environment variables

kubernetes won't have great encryption mechanism whenever we're creating the secrets by default use hashicorp vault and some sort of stuff
How to encrypt etcd for secrets

RBAC(Role Based access control) - directly related to security
users(iam-user and groups(IAM-OAUTH-PROVIDER))/service accounts(basiscally a kinda yaml file), roles/cluster-roles, role-binding/cluster-role-binding
role - to access a single namespace in k8s, we gonna create a role
cluster role - to access a whole cluster in k8s, we gonna create a cluster role
role binding - helps attaching a role(permissions) to service-account/user(user management)

kubernetes not going manage the users, it offloads to identity providers
person gets access to the kubernetes cluster without creating a user(if I download a example app, then it automatically asks login with googl/linkedin)

At a high level we will create a service account/user and then we'll create a role, using the role-binding we attach both(service-account/iuser with role)
suppose, if we're creating a role in a specifc namespace it is called as role, and if we're creating role in the scope of cluster it's called cluster-role, we can relate to role-binding as well

KUBERNETES MONITORING(USING PROMETHEUS AND GRAFANA)
k8s has API server, which exposes, this api server gonna exposes lot of metrics, PROMETHEUS gonna fetch this info and stores in TSDB(stores the metrics of k8s w.r.to timestamp)
PROMETHEUS gonna integrate with alert manager for sending emails(slack)
GRAFANA - used for data visualization, retrieves the info from PROMETHEUS, we can configure PROMETHEUS as a datasource
we can download the PROMETHEUS and GRAFANA using helm and operators
grafana id -3662


CRD - (custom resource defintion)

Daemonsets - if we wanna deploy system-services(backup-services) or monitoring services on each node as a pod we use daemonsets
JOBS - it should perform its duties on nodes and depart (kubectl logs <podname>)

HPA  - metric-server-plugin needs to be configured(in some clouds by default it's downloaded)(fixes the performance related issues(based on cpu/memory utilizaton, depending on resource utilization the pod should scaleout/scalein))

For any plugins we can watch out it in default kube-system(kubectl get all -n kube-system)


even HPA also has the separate manifest file


Cluster AUTOSCALER - scale out/in the number of nodes using the ASG but not a manifest file